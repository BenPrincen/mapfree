{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0471e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from silk_lib.common import SILK_MATCHER, get_model, load_images\n",
    "\n",
    "from silk.backbones.silk.silk import from_feature_coords_to_image_coords\n",
    "from silk.cli.image_pair_visualization import create_img_pair_visual, save_image\n",
    "from lib.unproject_points import unproject_points\n",
    "from lib.find_pose import find_relative_pose\n",
    "from torch.utils.data import DataLoader\n",
    "from yacs.config import CfgNode as CN\n",
    "import unittest\n",
    "from lib.dataset.mapfree import MapFreeDataset\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from lib.camera import Camera\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utils.data import data_to_model_device\n",
    "from lib.rot3 import Rot3\n",
    "from lib.pose3 import Pose3\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dcfd44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-31 15:14:58.103\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"../../silk/assets/models/silk/coco-rgb-aug.ckpt\"\n",
    "model = get_model(checkpoint=checkpoint, default_outputs=(\"sparse_positions\", \"sparse_descriptors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03d05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_config = \"../config/sift/sift_config.yaml\"\n",
    "checkpoint = \"../../silk/assets/models/silk/coco-rgb-aug.ckpt\"\n",
    "data_dir = \"../lib/tests/test_data\"\n",
    "dataset_config = os.path.join(data_dir, \"testset.yaml\")\n",
    "\n",
    "node = CN()\n",
    "node.set_new_allowed(True)\n",
    "node.merge_from_file(dataset_config)\n",
    "node.DEBUG = False\n",
    "\n",
    "# explicitely setting to None because if loading from yaml it's a string\n",
    "node.DATASET.SCENES = None\n",
    "node.DATASET.AUGMENTATION_TYPE = None\n",
    "node.DATASET.DATA_ROOT = os.path.join(\"..\", node.DATASET.DATA_ROOT)\n",
    "node.DATASET.DEPTH_ROOT = os.path.join(\"..\", node.DATASET.DEPTH_ROOT)\n",
    "dataset = MapFreeDataset(node, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3cec665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one(img1, img2, img1_depth, img2_depth, camera1, camera2, depth_scale, model) -> tuple:\n",
    "    grayscale = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert image to grayscale  \n",
    "    ])\n",
    "    grayscale_img1 = img1 if img1.shape[0] == 1 else grayscale(img1)\n",
    "    grayscale_img2 = img2 if img2.shape[0] == 1 else grayscale(img2)\n",
    "    grayscale_img1 = grayscale_img1.unsqueeze(0)\n",
    "    grayscale_img2 = grayscale_img2.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sparse_positions_1, sparse_descriptors_1 = model(grayscale_img1)\n",
    "        sparse_positions_2, sparse_descriptors_2 = model(grayscale_img2)        \n",
    "\n",
    "    sparse_positions_1 = from_feature_coords_to_image_coords(model, sparse_positions_1)\n",
    "    sparse_positions_2 = from_feature_coords_to_image_coords(model, sparse_positions_2)\n",
    "    \n",
    "    matches = SILK_MATCHER(sparse_descriptors_1[0], sparse_descriptors_2[0])\n",
    "\n",
    "    pts1 = sparse_positions_1[0][matches[:, 0]].detach().cpu().numpy()\n",
    "    pts2 = sparse_positions_2[0][matches[:, 1]].detach().cpu().numpy()\n",
    "    pts1 = pts1[:, :-1]\n",
    "    pts2 = pts2[:, :-1]\n",
    "\n",
    "\n",
    "    img1_depth = img1_depth.detach().cpu().numpy()\n",
    "    img2_depth = img2_depth.detach().cpu().numpy()\n",
    "    print(img1_depth.shape)\n",
    "    print(img1.shape)\n",
    "    pts1 = pts1[:, [1, 0]]\n",
    "    pts2 = pts2[:, [1, 0]]\n",
    "    print(pts1)\n",
    "    pts1_3d = unproject_points(pts1, img1_depth, camera1, depth_scale)\n",
    "    pts2_3d = unproject_points(pts2, img2_depth, camera2, depth_scale)\n",
    "\n",
    "    (R, t), inliers = find_relative_pose(\n",
    "        pts1_3d,\n",
    "        pts2_3d,\n",
    "        ransac_iterations=100,\n",
    "        inlier_threshold=0.15,\n",
    "        num_matches=3,\n",
    "    )\n",
    "    return R, t, inliers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0f54520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width: tensor([540.], device='cuda:0', dtype=torch.float64)\n",
      "(720, 540)\n",
      "torch.Size([3, 720, 540])\n",
      "[[ 41.5 218.5]\n",
      " [ 51.5 219.5]\n",
      " [ 52.5 219.5]\n",
      " [ 47.5 220.5]\n",
      " [105.5 221.5]\n",
      " [105.5 223.5]\n",
      " [ 19.5 224.5]\n",
      " [ 20.5 224.5]\n",
      " [134.5 238.5]\n",
      " [134.5 239.5]\n",
      " [ 15.5 248.5]\n",
      " [ 18.5 248.5]\n",
      " [ 64.5 255.5]\n",
      " [ 16.5 256.5]\n",
      " [211.5 257.5]\n",
      " [ 60.5 260.5]\n",
      " [ 61.5 260.5]\n",
      " [ 60.5 261.5]\n",
      " [ 61.5 262.5]\n",
      " [241.5 265.5]\n",
      " [173.5 284.5]\n",
      " [175.5 284.5]\n",
      " [172.5 285.5]\n",
      " [ 40.5 299.5]\n",
      " [ 26.5 300.5]\n",
      " [315.5 332.5]\n",
      " [273.5 338.5]\n",
      " [154.5 347.5]\n",
      " [296.5 380.5]\n",
      " [263.5 381.5]\n",
      " [203.5 388.5]\n",
      " [472.5 475.5]\n",
      " [362.5 507.5]\n",
      " [ 98.5 518.5]\n",
      " [ 99.5 518.5]]\n",
      "width: tensor([540.], device='cuda:0', dtype=torch.float64)\n",
      "(720, 540)\n",
      "torch.Size([3, 720, 540])\n",
      "[[ 69.5 202.5]\n",
      " [ 69.5 203.5]\n",
      " [ 69.5 204.5]\n",
      " [288.5 205.5]\n",
      " [ 41.5 219.5]\n",
      " [ 50.5 219.5]\n",
      " [ 19.5 226.5]\n",
      " [134.5 239.5]\n",
      " [ 14.5 249.5]\n",
      " [ 62.5 253.5]\n",
      " [210.5 255.5]\n",
      " [211.5 255.5]\n",
      " [ 33.5 257.5]\n",
      " [ 33.5 258.5]\n",
      " [173.5 284.5]\n",
      " [174.5 284.5]\n",
      " [172.5 285.5]\n",
      " [174.5 286.5]\n",
      " [159.5 288.5]\n",
      " [247.5 339.5]\n",
      " [232.5 341.5]\n",
      " [233.5 341.5]\n",
      " [222.5 342.5]\n",
      " [233.5 342.5]\n",
      " [195.5 378.5]\n",
      " [296.5 379.5]\n",
      " [263.5 381.5]\n",
      " [483.5 387.5]\n",
      " [219.5 388.5]\n",
      " [483.5 388.5]\n",
      " [203.5 390.5]\n",
      " [ 72.5 410.5]\n",
      " [135.5 483.5]\n",
      " [503.5 691.5]]\n",
      "width: tensor([540.], device='cuda:0', dtype=torch.float64)\n",
      "(720, 540)\n",
      "torch.Size([3, 720, 540])\n",
      "[[257.5 197.5]\n",
      " [ 69.5 202.5]\n",
      " [ 69.5 203.5]\n",
      " [ 70.5 203.5]\n",
      " [ 69.5 204.5]\n",
      " [ 50.5 219.5]\n",
      " [ 47.5 220.5]\n",
      " [ 19.5 224.5]\n",
      " [133.5 241.5]\n",
      " [134.5 241.5]\n",
      " [134.5 242.5]\n",
      " [135.5 242.5]\n",
      " [166.5 245.5]\n",
      " [244.5 246.5]\n",
      " [ 14.5 249.5]\n",
      " [ 16.5 249.5]\n",
      " [ 17.5 249.5]\n",
      " [ 18.5 249.5]\n",
      " [ 76.5 252.5]\n",
      " [ 62.5 253.5]\n",
      " [ 33.5 257.5]\n",
      " [ 35.5 257.5]\n",
      " [ 33.5 258.5]\n",
      " [ 34.5 258.5]\n",
      " [ 35.5 258.5]\n",
      " [ 34.5 259.5]\n",
      " [ 61.5 261.5]\n",
      " [ 62.5 261.5]\n",
      " [175.5 281.5]\n",
      " [361.5 330.5]\n",
      " [260.5 339.5]\n",
      " [222.5 341.5]\n",
      " [142.5 349.5]\n",
      " [220.5 387.5]\n",
      " [205.5 389.5]\n",
      " [ 89.5 441.5]]\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "for data in loader:\n",
    "    data = data_to_model_device(data, model)\n",
    "    img1 = data[\"image0\"].squeeze()\n",
    "    img2 = data[\"image1\"].squeeze()\n",
    "    img1_depth = data[\"depth0\"].squeeze()\n",
    "    img2_depth = data[\"depth1\"].squeeze()\n",
    "    \n",
    "    K1 = data[\"K_color0\"].detach().cpu().numpy().squeeze()\n",
    "    K2 = data[\"K_color1\"].detach().cpu().numpy().squeeze()\n",
    "    camera1 = Camera.from_K(K1, img1.shape[1], img1.shape[0])\n",
    "    camera2 = Camera.from_K(K2, img2.shape[1], img2.shape[0])\n",
    "    frame_num = data[\"pair_names\"][1][0][-9:-4]\n",
    "    R, t, inliers = run_one(img1, img2, img1_depth, img2_depth, camera1, camera2, 1.0, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76589ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 720, 540])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert image to grayscale  \n",
    "])\n",
    "grayscale_img1 = grayscale(img1).unsqueeze(0)\n",
    "grayscale_img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e635149",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_positions_1, sparse_descriptors_1 = model(grayscale_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "282cb6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10001, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_positions_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae01846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilkRunner:\n",
    "    def __init__(self, config: str, checkpoint: str) -> None:\n",
    "        self._config = CN()\n",
    "        self._config.set_new_allowed(True)\n",
    "        if os.path.exists(config):\n",
    "            self._config.merge_from_file(config)\n",
    "        if os.path.exists(checkpoint):\n",
    "            print(\"Found checkpoint path.\")\n",
    "            self._model = get_model(\n",
    "                checkpoint=checkpoint, default_outputs=(\"sparse_positions\", \"sparse_descriptors\")\n",
    "            )\n",
    "        else:\n",
    "            print(\"Didn't find checkpoint path. Using default.\")\n",
    "            self._model = get_model(\n",
    "                default_outputs=(\"sparse_positions\", \"sparse_descriptors\")\n",
    "            ) # use default otherwise\n",
    "            \n",
    "        self._model.eval()\n",
    "            \n",
    "    def run_one(self, img1, img2, img1_depth, img2_depth, camera1, camera2, depth_scale) -> tuple:\n",
    "        # convert to grayscale first\n",
    "        grayscale = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),  # Convert image to grayscale  \n",
    "        ])\n",
    "        grayscale_img1 = img1 if img1.shape[0] == 1 else grayscale(img1)\n",
    "        grayscale_img2 = img2 if img2.shape[0] == 1 else grayscale(img2)\n",
    "        grayscale_img1 = grayscale_img1.unsqueeze(0)\n",
    "        grayscale_img2 = grayscale_img2.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sparse_positions_1, sparse_descriptors_1 = self._model(grayscale_img1)\n",
    "            sparse_positions_2, sparse_descriptors_2 = self._model(grayscale_img2)        \n",
    "\n",
    "        sparse_positions_1 = from_feature_coords_to_image_coords(self._model, sparse_positions_1)\n",
    "        sparse_positions_2 = from_feature_coords_to_image_coords(self._model, sparse_positions_2)\n",
    "        \n",
    "        matches = SILK_MATCHER(sparse_descriptors_1[0], sparse_descriptors_2[0])\n",
    "        \n",
    "        pts1 = sparse_positions_1[0][matches[:, 0]].detach().cpu().numpy()\n",
    "        pts2 = sparse_positions_2[0][matches[:, 1]].detach().cpu().numpy()\n",
    "        pts1 = pts1[:, :-1]\n",
    "        pts2 = pts2[:, :-1]\n",
    "        \n",
    "        img1_depth = img1_depth.detach().cpu().numpy()\n",
    "        img2_depth = img2_depth.detach().cpu().numpy()\n",
    "        pts1 = pts1[:, [1, 0]]\n",
    "        pts2 = pts2[:, [1, 0]]\n",
    "        pts1_3d = unproject_points(pts1, img1_depth, camera1, depth_scale)\n",
    "        pts2_3d = unproject_points(pts2, img2_depth, camera2, depth_scale)\n",
    "        \n",
    "        (R, t), inliers = find_relative_pose(\n",
    "            pts1_3d,\n",
    "            pts2_3d,\n",
    "            ransac_iterations=self._config.SIFT.RANSAC_ITERATIONS,\n",
    "            inlier_threshold=self._config.SIFT.INLIER_THRESHOLD,\n",
    "            num_matches=self._config.SIFT.NUM_MATCHES,\n",
    "        )\n",
    "        return R, t, inliers\n",
    "        \n",
    "\n",
    "    def run(self, data_loader: DataLoader) -> dict:\n",
    "        estimated_poses = defaultdict(list)\n",
    "        for data in data_loader:\n",
    "            data = data_to_model_device(data, self._model)\n",
    "            img1 = data[\"image0\"].squeeze()\n",
    "            img2 = data[\"image1\"].squeeze()\n",
    "            img1_depth = data[\"depth0\"].squeeze()\n",
    "            img2_depth = data[\"depth1\"].squeeze()\n",
    "            camera1 = Camera.from_K(data[\"K_color0\"].detach().cpu().numpy().squeeze(), img1.shape[1], img1.shape[0])\n",
    "            camera2 = Camera.from_K(data[\"K_color1\"].detach().cpu().numpy().squeeze(), img2.shape[1], img2.shape[0])\n",
    "            frame_num = data[\"pair_names\"][1][0][-9:-4]\n",
    "            scene = data[\"scene_id\"][0]\n",
    "            print(f\"scene: {scene}\")\n",
    "            R, t, inliers = self.run_one(img1, img2, img1_depth, img2_depth, camera1, camera2, depth_scale=1.0)\n",
    "            r = Rot3(R.squeeze())\n",
    "            estimated_pose = (Pose3(r, t), float(inliers), int(frame_num))\n",
    "            estimated_poses[scene].append(estimated_pose)\n",
    "        return estimated_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6170a758",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_creation (__main__.TestSilkRunner) ... \u001b[32m2024-07-31 15:45:14.599\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n",
      "\u001b[32m2024-07-31 15:45:14.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n",
      "ok\n",
      "test_run (__main__.TestSilkRunner) ... \u001b[32m2024-07-31 15:45:14.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n",
      "\u001b[32m2024-07-31 15:45:14.753\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint path.\n",
      "Found checkpoint path.\n",
      "scene: s00460\n",
      "scene: s00460\n",
      "scene: s00460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_run_one (__main__.TestSilkRunner) ... \u001b[32m2024-07-31 15:45:15.211\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n",
      "\u001b[32m2024-07-31 15:45:15.254\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mfsspec.implementations.local\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m347\u001b[0m - \u001b[34m\u001b[1mopen file: /home/vgmachinist/Desktop/Projects/mapfree/notebooks/../../silk/assets/models/silk/coco-rgb-aug.ckpt\u001b[0m\n",
      "ok"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.864s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7311a8091910>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestSilkRunner(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUp(cls):\n",
    "        \n",
    "        def initConfig(dataset_config: str) -> CN:\n",
    "            node = CN()\n",
    "            node.set_new_allowed(True)\n",
    "            node.merge_from_file(dataset_config)\n",
    "            node.DEBUG = False\n",
    "            \n",
    "            # explicitely setting to None because if loading from yaml it's a string\n",
    "            node.DATASET.SCENES = None\n",
    "            node.DATASET.AUGMENTATION_TYPE = None\n",
    "            return node\n",
    "        \n",
    "        cls.sift_config = \"../config/sift/sift_config.yaml\"\n",
    "        cls.checkpoint = \"../../silk/assets/models/silk/coco-rgb-aug.ckpt\"\n",
    "        cls.data_dir = \"../lib/tests/test_data\"\n",
    "        cls.dataset_config = os.path.join(cls.data_dir, \"testset.yaml\")\n",
    "        \n",
    "        paths = [cls.sift_config, cls.checkpoint, cls.data_dir, cls.dataset_config]\n",
    "        paths_exist = [\n",
    "            os.path.exists(i) for i in paths\n",
    "        ]\n",
    "        \n",
    "        if sum(paths_exist) < len(paths): \n",
    "            print(\"Not all paths exist :(\")\n",
    "            exit(1) # exit failure\n",
    "            \n",
    "        cls.config = initConfig(cls.dataset_config)\n",
    "        cls.config.DATASET.DATA_ROOT = os.path.join(\"..\", cls.config.DATASET.DATA_ROOT)\n",
    "        cls.config.DATASET.DEPTH_ROOT = os.path.join(\"..\", cls.config.DATASET.DEPTH_ROOT)\n",
    "        cls.dataset = MapFreeDataset(cls.config, \"val\")\n",
    "        cls.model = get_model(checkpoint=cls.checkpoint, default_outputs=(\"sparse_positions\", \"sparse_descriptors\"))\n",
    "    \n",
    "    def test_creation(self):\n",
    "        silk_runner = SilkRunner(self.sift_config, self.checkpoint)\n",
    "        self.assertTrue(isinstance(silk_runner, SilkRunner))\n",
    "        self.assertTrue(isinstance(self.dataset, MapFreeDataset))\n",
    "        \n",
    "    def test_run_one(self):\n",
    "        silk_runner = SilkRunner(self.sift_config, self.checkpoint)\n",
    "        data = data_to_model_device(self.dataset[0], self.model)\n",
    "        img1 = data[\"image0\"]\n",
    "        img2 = data[\"image1\"]\n",
    "        depth1 = data[\"depth0\"]\n",
    "        depth2 = data[\"depth1\"]\n",
    "        camera1 = Camera.from_K(data[\"K_color0\"].detach().cpu().numpy(), img1.shape[1], img1.shape[0])\n",
    "        camera2 = Camera.from_K(data[\"K_color1\"].detach().cpu().numpy(), img2.shape[1], img2.shape[0])\n",
    "        R, t, inliers = silk_runner.run_one(\n",
    "            img1, img2, depth1, depth2, camera1, camera2, depth_scale=1.0\n",
    "        )\n",
    "        self.assertEqual(R.shape, (3, 3))\n",
    "        self.assertEqual(t.shape, (3,))\n",
    "        self.assertTrue(inliers > 0)\n",
    "        \n",
    "    def test_run(self):\n",
    "        silk_runner = SilkRunner(self.sift_config, self.checkpoint)\n",
    "        self.assertTrue(silk_runner)\n",
    "        self.assertTrue(self.dataset)\n",
    "        self.assertTrue(self.sift_config)\n",
    "        \n",
    "        loader = DataLoader(self.dataset, batch_size=1)\n",
    "        estimated_poses = silk_runner.run(loader)\n",
    "        self.assertEqual(len(estimated_poses), 1)\n",
    "\n",
    "        for k, v in estimated_poses.items():\n",
    "            for pose_info in v:\n",
    "                pose, inliers, frame_num = pose_info\n",
    "                self.assertTrue(isinstance(pose, Pose3))\n",
    "                self.assertTrue(isinstance(inliers, float))\n",
    "                self.assertTrue(isinstance(frame_num, int))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534df46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_0_PATH = \"../data/val/s00460/seq1/frame_00000.jpg\"\n",
    "IMAGE_1_PATH = \"../data/val/s00460/seq1/frame_00001.jpg\"\n",
    "\n",
    "OUTPUT_IMAGE_PATH = \"./img.png\"\n",
    "images_0 = load_images(IMAGE_0_PATH)\n",
    "images_1 = load_images(IMAGE_1_PATH)\n",
    "\n",
    "# load model\n",
    "model = get_model(default_outputs=(\"sparse_positions\", \"sparse_descriptors\"))\n",
    "\n",
    "# run model\n",
    "sparse_positions_0, sparse_descriptors_0 = model(images_0)\n",
    "sparse_positions_1, sparse_descriptors_1 = model(images_1)\n",
    "\n",
    "sparse_positions_0 = from_feature_coords_to_image_coords(model, sparse_positions_0)\n",
    "sparse_positions_1 = from_feature_coords_to_image_coords(model, sparse_positions_1)\n",
    "\n",
    "# get matches\n",
    "matches = SILK_MATCHER(sparse_descriptors_0[0], sparse_descriptors_1[0])\n",
    "\n",
    "# create output image\n",
    "image_pair = create_img_pair_visual(\n",
    "    IMAGE_0_PATH,\n",
    "    IMAGE_1_PATH,\n",
    "    None,\n",
    "    None,\n",
    "    sparse_positions_0[0][matches[:, 0]].detach().cpu().numpy(),\n",
    "    sparse_positions_1[0][matches[:, 1]].detach().cpu().numpy(),\n",
    ")\n",
    "\n",
    "save_image(\n",
    "    image_pair,\n",
    "    os.path.dirname(OUTPUT_IMAGE_PATH),\n",
    "    os.path.basename(OUTPUT_IMAGE_PATH),\n",
    ")\n",
    "\n",
    "print(f\"result saved in {OUTPUT_IMAGE_PATH}\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
